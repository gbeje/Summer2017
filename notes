6/20/2017
options for research direction:
- cues (compare to each other? gaze, lights on head or arms, visualizations, verbal)
- "mistakes" (using the "wrong" cue and seeing how that affects the user)
- 

display possibilities:
- text
- images (eg baxter with right hand up if right hand will move)

notes based on reading:
*seems that gender has a lot to do with social interactions 
   - seems that studies note (and usually try to have an equal number of each 
or constrict their research to one gender) and then applies to robot. 
*HRI/collaboration/handoffs have a lot of motion equations 
   - apparently big effect on human experience
*statistical analysis ranging from simple (eg means) to more complicated 
(eg ANCOVA)
*lots of robots copying how humans interact... i have not found new communications
based on abilities that humans don't have (such as lights)
	- found one about non-humanid robots, lights and sounds 
---------
6/21/2017
wrote a publisher to display images on baxter screen
- sometimes works sometimes doesn't
      works when the message is published multiple times (ie in a loop)

display camera on screen
- works but does not update fast (at all)
- ssh into baxter (the proper way) might work better?
	will try that later

is it possible to turn head towards person? (using sonar to detect location)
- can have multiple people/objects within range...

rostopic echo /robot/sonar_states 
ERROR: Cannot load message class for [MotorControlMsgs/SonarStates]. Are your messages built?


"You can enable or disable sensors by publishing to the topic  "/robot/sonar/head_sonar/set_sonars_enabled".  A message of 4095 (0b0000111111111111) will turn all the sensors on, while sending a message of 3 (0b0000000000000011) would only turn off all sensors except sensors 1 and 2.  Also, make sure to latch the message."
(https://groups.google.com/a/rethinkrobotics.com/forum/?utm_medium=email&utm_source=footer#!topic/brr-users/QoDMdvCgUA4)



http://wiki.ros.org/roscpp/Overview/Publishers%20and%20Subscribers
"Intraprocess Publishing

When a publisher and subscriber to the same topic both exist inside the same node, roscpp can skip the serialize/deserialize step (potentially saving a large amount of processing and latency). It can only do this though if the message is published as a shared_ptr:"
"Note that when publishing in this fashion, there is an implicit contract between you and roscpp: you may not modify the message you've sent after you send it, since that pointer will be passed directly to any intraprocess subscribers. If you want to send another message, you must allocate a new one and send that."
^ might be useful for making video appear faster

"queue_size [required]

    This is the size of the outgoing message queue. If you are publishing faster than roscpp can send the messages over the wire, roscpp will start dropping old messages. A value of 0 here means an infinite queue, which can be dangerous. See the rospy documentation on choosing a good queue_size for more information."
^also good for video

---------
6/22/2017 image_transport::ImageTransport it(n);
"Baxter's eyes also move in the direction one of its arms is about to move as a warning to anyone working nearby." (industrial baxter)
http://science.howstuffworks.com/baxter-robot1.htm

maybe use suction instead of grippers? 

moving screen towards sonar:
sliding window/rolling window
probability 

collaboration deception

---------
6/23/2017

working on panning head towards closest object
all coded, publishing to correct topic, but head not moving...
robot wasn't enabled...

expire the minimum after a certain amount of time using timestamps to compare


http://sdk.rethinkrobotics.com/wiki/API_Reference#head-joints
The points for all incoming, valid, current readings are collected each time and published as an array in the PointCloud message.
Note: the result of this is that each published message contains measurements for only a subset of the sensors, not readings for all 12 sensors every time.
There will be a Point for each currently active measurement - this means a message published at one instant could contain points for only 4 of the sensors, and, also, another instant the published message could contain two points for the same sensor if the sensor had two valid measurements in the last time interval. 

---------
6/26/2017
expiring can be problematic too: if the person stays in front of the same sonar
sensor, and that sensor expires, then the screen moves away from the target.

ideas to improve screen following person:
- if a distance expires, look for min in array instead of starting over
	- have an array of only 7 (instead of all) sensors numbered in order

 3  2  1 12 11 10  9	if >6, -12 
 3  2  1 0  -1 -2 -3	*(-1)
-3 -2 -1 0   1  2  3	+3
 0  1  2 3   4  5  6	done 
to use for target position
PI/6*(index-3)
all together: (if >6, -12)*(-1)+3

problem with finding minimum after expiration instead of starting over:
time stamps are not stored, no easy way to (so far, index is sensor#, data
is the distance, so timestamp is ...?)
making second array with same indexing system to hold time stamps

---------
6/27/2017
enabling baxter just kept running without finishing or even displaying any 
errors. solution was to turn it off and turn it back on again (ew). 

looking at dictionaries in c++ and python.

looking at complacency & trust/deception. 

---------
6/28/2017
pan_head_node: forgot to update min_stamp when the same sensor detects a 
farther distance. 

going to:
x	copy all _node fixes to _smallarray_node
x	make _smallarray_node work
x	make _smallarray_node search for minimum if expired or if same 
		sensor gets a larger distance (instead of starting over)
	^ doesn't make sense to have the same function. if expired, then
		if there are no values in the array, reset. if just the same 
		sensor, then there should be values in the array actually...
		needs more looking
	* function thinks that no recent measurements were found. the distances
		array has valid measurements. the timestamp array has valid 
		measurements
	|-----> problem was that the distance had to be less than the minimum,
		but the minimum in the array would EQUAL the current minimum,
		so it reset
idea: 
	instead of expiring based solely on time, can also count how many
		messages have been received since then. if the time limit and the
		counting limit are both exceeded, only then expire
	^ why? in case that the next message processed already expires the 
		minimum. The subscriber only has a queue of 1 so there is
		potential for a large time gap
	^ does this work with the array? does the number of times since last
		received also need to be tracked?
idea: 
	instead of using neighbors as equivalent for the counter, if (sensor for 
		new minimum distance is the same as the previous minimum or it is 
		its neighbor AND count < limit) OR (count >= limit and sensor is 
		the same as the previous minimum) then count++ (else count=0) 
for future:
	make it so that no commands to move head are published while Baxter 
		moves? (so that it doesn't see its own arm and then turn to 
		face it) (or does looking at its arm make the motion 
		seem intentional?)

---------
6/29/2017
going to leave the head-following as is (just playing with numbers at this 
point...)
next up: grippers

testing code written before this computer (not tested on physical baxter - 
didn't work in sim)
calibrate doesn't even get sent?
maybe need to keep trying... good place for service and client? 

installing moveit:
installation worked fine but now rosrun doesn't recognize any of the packages
(even the ones from before that were already run, such as examples).
need to re-source apparently (after installation, sourced source 
/opt/ros/indigo/setup.bash may have been in the wrong directory). 
source ~/.bashrc worked. 

trying to run moveit ik code: 
Robot semantic description not found. Did you forget to define or remap '/robot_description_semantic'?
restarting terminal
following instructions from rethink robotics 
(http://sdk.rethinkrobotics.com/wiki/MoveIt_Tutorial)
^ worked
Rviz takes a long time to work, can't even plan using GUI... Ryan will continue
using the Baxter IK service

putting together arm movement and gripping. successfully moves to hard-coded point, 
closes grippers. successfully moves to (numerical) "start pose" (hard coded from
echo). unsuccessful when setting start pose point and orientation in the node by 
subscribing to (and copying) the same topic

---------
6/30/2017
splitting the all together (moving arms) into services

following instructions to make new srv file but cant show it with rossrv show __
not sure why.
sourced /opt/ros/indigo/setup.bash again. didn't fix.
source devel/setup.bash FIXED

same instructions again (srv)
neither one of the new AddTwoInts is showing up with rossrv show AddTwoInts (only
rospy_tutorials version shows up). Was working before...

Asked Carlos for help, larger problem than I was seeing... whole workspace had 
issues. recommendation: use a new workspace for anything not baxter (use catkin
tools). use separate workspace for baxter (use catkin_make). 

--------
7/6/2017
WARNING: Your current environment's CMAKE_PREFIX_PATH is
different from the cached CMAKE_PREFIX_PATH used the last
time this workspace was built.

If you want to use a different CMAKE_PREFIX_PATH you should
call `catkin clean` to remove all references to the previous
CMAKE_PREFIX_PATH.

Cached CMAKE_PREFIX_PATH:
	/opt/ros/indigo
Current CMAKE_PREFIX_PATH:
	/opt/ros/indigo:/home/csrobot/gal_sandbox/devel

not doing anything... looks like something Carlos pointed out while analyzing the 
situation and diagnosing the problem... but I'll just leave it for now

brainstorming with Ryan about end goal: 
MAKE SANDWICH: robot gets an order, makes it with human coworker. 
 - what happens if robot just presents sandwich in progress to coworker? do they 
	know what to do?
*- what if robot tells coworker to add ingredient but never stops/gives time for 
	that to happen? compare to stopping and then moving after x time (eg 3 seconds), 
	stopping and displaying a timer of how long until robot moves again, etc. 
 - could theoretically just use positions (normal. in cafeterias, ingredients are 
	in bins in the same spot)
 - different ways to tell coworker what to do
	 - screen, pointing, nothing
 - maybe make human coworker take care of ingredients that we can't get robot to pick up 
	(ie single slice of cheese from a stack)
 - maybe coworker has to refill bins that are empty (maybe a side task to distract
	them?) to test different ways of getting their attention and instructing
	them. 
 - there doesn't seem to be a way of publishing text to the baxter screen, unfortunately
for arm moving service: node will have to specify which arm (L/R) and pose

setting up baxter_workspace (with catkin_make)



options for research direction:
- cues (compare to each other? gaze, lights on head or arms, visualizations, verbal)
- "mistakes" (using the "wrong" cue and seeing how that affects the user)
- 

display possibilities:
- text
- images (eg baxter with right hand up if right hand will move)

notes based on reading:
*seems that gender has a lot to do with social interactions 
   - seems that studies note (and usually try to have an equal number of each 
or constrict their research to one gender) and then applies to robot. 
*HRI/collaboration/handoffs have a lot of motion equations 
   - apparently big effect on human experience
*statistical analysis ranging from simple (eg means) to more complicated 
(eg ANCOVA)
*lots of robots copying how humans interact... i have not found new communications
based on abilities that humans don't have (such as lights)
	- found one about non-humanid robots, lights and sounds 
---------
6/21/2017
wrote a publisher to display images on baxter screen
- sometimes works sometimes doesn't
      works when the message is published multiple times (ie in a loop)

display camera on screen
- works but does not update fast (at all)
- ssh into baxter (the proper way) might work better?
	will try that later

is it possible to turn head towards person? (using sonar to detect location)
- can have multiple people/objects within range...

rostopic echo /robot/sonar_states 
ERROR: Cannot load message class for [MotorControlMsgs/SonarStates]. Are your messages built?


"You can enable or disable sensors by publishing to the topic  "/robot/sonar/head_sonar/set_sonars_enabled".  A message of 4095 (0b0000111111111111) will turn all the sensors on, while sending a message of 3 (0b0000000000000011) would only turn off all sensors except sensors 1 and 2.  Also, make sure to latch the message."
(https://groups.google.com/a/rethinkrobotics.com/forum/?utm_medium=email&utm_source=footer#!topic/brr-users/QoDMdvCgUA4)



http://wiki.ros.org/roscpp/Overview/Publishers%20and%20Subscribers
"Intraprocess Publishing

When a publisher and subscriber to the same topic both exist inside the same node, roscpp can skip the serialize/deserialize step (potentially saving a large amount of processing and latency). It can only do this though if the message is published as a shared_ptr:"
"Note that when publishing in this fashion, there is an implicit contract between you and roscpp: you may not modify the message you've sent after you send it, since that pointer will be passed directly to any intraprocess subscribers. If you want to send another message, you must allocate a new one and send that."
^ might be useful for making video appear faster

"queue_size [required]

    This is the size of the outgoing message queue. If you are publishing faster than roscpp can send the messages over the wire, roscpp will start dropping old messages. A value of 0 here means an infinite queue, which can be dangerous. See the rospy documentation on choosing a good queue_size for more information."
^also good for video

---------
6/22/2017
"Baxter's eyes also move in the direction one of its arms is about to move as a warning to anyone working nearby." (industrial baxter)
http://science.howstuffworks.com/baxter-robot1.htm

maybe use suction instead of grippers? 

moving screen towards sonar:
sliding window/rolling window
probability 

collaboration deception

---------
6/23/2017

working on panning head towards closest object
all coded, publishing to correct topic, but head not moving...
robot wasn't enabled...

expire the minimum after a certain amount of time using timestamps to compare


http://sdk.rethinkrobotics.com/wiki/API_Reference#head-joints
The points for all incoming, valid, current readings are collected each time and published as an array in the PointCloud message.
Note: the result of this is that each published message contains measurements for only a subset of the sensors, not readings for all 12 sensors every time.
There will be a Point for each currently active measurement - this means a message published at one instant could contain points for only 4 of the sensors, and, also, another instant the published message could contain two points for the same sensor if the sensor had two valid measurements in the last time interval. 

---------
6/26/2017
expiring can be problematic too: if the person stays in front of the same sonar
sensor, and that sensor expires, then the screen moves away from the target.

ideas to improve screen following person:
- if a distance expires, look for min in array instead of starting over
	- have an array of only 7 (instead of all) sensors numbered in order

 3  2  1 12 11 10  9	if >6, -12 
 3  2  1 0  -1 -2 -3	*(-1)
-3 -2 -1 0   1  2  3	+3
 0  1  2 3   4  5  6	done 
to use for target position
PI/6*(index-3)
all together: (if >6, -12)*(-1)+3

problem with finding minimum after expiration instead of starting over:
time stamps are not stored, no easy way to (so far, index is sensor#, data
is the distance, so timestamp is ...?)
making second array with same indexing system to hold time stamps

---------
6/27/2017
enabling baxter just kept running without finishing or even displaying any 
errors. solution was to turn it off and turn it back on again (ew). 

looking at dictionaries in c++ and python.

looking at complacency & trust/deception. 

---------
6/28/2017
pan_head_node: forgot to update min_stamp when the same sensor detects a 
farther distance. 

going to:
x	copy all _node fixes to _smallarray_node
x	make _smallarray_node work
x	make _smallarray_node search for minimum if expired or if same 
		sensor gets a larger distance (instead of starting over)
	^ doesn't make sense to have the same function. if expired, then
		if there are no values in the array, reset. if just the same 
		sensor, then there should be values in the array actually...
		needs more looking
	* function thinks that no recent measurements were found. the distances
		array has valid measurements. the timestamp array has valid 
		measurements
	|-----> problem was that the distance had to be less than the minimum,
		but the minimum in the array would EQUAL the current minimum,
		so it reset
idea: 
	instead of expiring based solely on time, can also count how many
		messages have been received since then. if the time limit and the
		counting limit are both exceeded, only then expire
	^ why? in case that the next message processed already expires the 
		minimum. The subscriber only has a queue of 1 so there is
		potential for a large time gap
	^ does this work with the array? does the number of times since last
		received also need to be tracked?
idea: 
	instead of using neighbors as equivalent for the counter, if (sensor for 
		new minimum distance is the same as the previous minimum or it is 
		its neighbor AND count < limit) OR (count >= limit and sensor is 
		the same as the previous minimum) then count++ (else count=0) 
for future:
	make it so that no commands to move head are published while Baxter 
		moves? (so that it doesn't see its own arm and then turn to 
		face it) (or does looking at its arm make the motion 
		seem intentional?)

---------
6/29/2017
going to leave the head-following as is (just playing with numbers at this 
point...)
next up: grippers

testing code written before this computer (not tested on physical baxter - 
didn't work in sim)
calibrate doesn't even get sent?
maybe need to keep trying... good place for service and client? 

installing moveit:
installation worked fine but now rosrun doesn't recognize any of the packages
(even the ones from before that were already run, such as examples).
need to re-source apparently (after installation, sourced source 
/opt/ros/indigo/setup.bash may have been in the wrong directory). 
source ~/.bashrc worked. 

trying to run moveit ik code: 
Robot semantic description not found. Did you forget to define or remap '/robot_description_semantic'?
restarting terminal
following instructions from rethink robotics 
(http://sdk.rethinkrobotics.com/wiki/MoveIt_Tutorial)
^ worked
Rviz takes a long time to work, can't even plan using GUI... Ryan will continue
using the Baxter IK service

putting together arm movement and gripping. successfully moves to hard-coded point, 
closes grippers. successfully moves to (numerical) "start pose" (hard coded from
echo). unsuccessful when setting start pose point and orientation in the node by 
subscribing to (and copying) the same topic

---------
6/30/2017
splitting the all together (moving arms) into services

following instructions to make new srv file but cant show it with rossrv show __
not sure why.
sourced /opt/ros/indigo/setup.bash again. didn't fix.
source devel/setup.bash FIXED

same instructions again (srv)
neither one of the new AddTwoInts is showing up with rossrv show AddTwoInts (only
rospy_tutorials version shows up). Was working before...

Asked Carlos for help, larger problem than I was seeing... whole workspace had 
issues. recommendation: use a new workspace for anything not baxter (use catkin
tools). use separate workspace for baxter (use catkin_make). 

--------
7/6/2017
WARNING: Your current environment's CMAKE_PREFIX_PATH is
different from the cached CMAKE_PREFIX_PATH used the last
time this workspace was built.

If you want to use a different CMAKE_PREFIX_PATH you should
call `catkin clean` to remove all references to the previous
CMAKE_PREFIX_PATH.

Cached CMAKE_PREFIX_PATH:
	/opt/ros/indigo
Current CMAKE_PREFIX_PATH:
	/opt/ros/indigo:/home/csrobot/gal_sandbox/devel

not doing anything... looks like something Carlos pointed out while analyzing the 
situation and diagnosing the problem... but I'll just leave it for now

brainstorming with Ryan about end goal: 
MAKE SANDWICH: robot gets an order, makes it with human coworker. 
 - what happens if robot just presents sandwich in progress to coworker? do they 
	know what to do?
*- what if robot tells coworker to add ingredient but never stops/gives time for 
	that to happen? compare to stopping and then moving after x time (eg 3 seconds), 
	stopping and displaying a timer of how long until robot moves again, having the 
	user specify when they are done (pressing a button), etc. 
 - could theoretically just use positions (normal. in cafeterias, ingredients are 
	in bins in the same spot)
 - different ways to tell coworker what to do
	 - screen, pointing, nothing
 - maybe make human coworker take care of ingredients that we can't get robot to pick up 
	(ie single slice of cheese from a stack)
 - maybe coworker has to refill bins that are empty (maybe a side task to distract
	them?) to test different ways of getting their attention and instructing
	them. 
 - there doesn't seem to be a way of publishing text to the baxter screen, unfortunately
(added 7/7/2017)
- https://www.theverge.com/2011/12/13/2631858/autonomous-sandwich-making-robot
^ use a fork to stab the foods and use other hand's spoon to push off
- what if human coworker has a stop button? meant for if robot thinks it places an ingredient
	but dropped it, so human needs to fix it. would the button be used? would it be overused?
	compare to no button?

for arm moving service: node will have to specify which arm (L/R) and pose

setting up baxter_workspace (with catkin_make)

is it possible to call a service from a function? trying to make a service that is a client for 
the IK service but also moves the arm to that position
	http://answers.ros.org/question/197868/can-a-node-be-a-subscriber-and-client-at-the-same-time/
^uses pointer to client
^Jordan recommends: "setting up the service call with a waitforservice in your main, then just call it"

Going to make 2 service, one for right and one for left

left service works! called by client successfully
right service also works! called by client successfully

starting to rewrite what Ryan was testing using the service
going back to the starting pose doesn't work

--------
7/7/2017
issue seems to be with how the client calls the service when the pose is not hard-coded,
not with the service when it handles more than 1 request (tested two hard-coded).

problem was that the client node did not subscribe to the endpoint, so the pose being 
given to the client was 0,0,0,0,0,0,0. fixed and made a version for right arm too. on 
git.

https://www.theverge.com/2011/12/13/2631858/autonomous-sandwich-making-robot
^ use a fork to stab the foods and use other hand's spoon to push off
what if human coworker has a stop button? meant for if robot thinks it places an ingredient
but dropped it, so human needs to fix it. would the button be used? would it be overused?
compare to no button? (adding to big list) 

recorded gripper ideas based on silverware (drawn on paper) 

investigating orientation constraint (plan the motion but with orientation of gripper 
not changing)

installing moveit

rviz not opening properly (cant use roslaunch and cant open robot model using 
rosrun rviz rviz)

sourced devel/setup.bash and roslaunch worked (slowly)
rosrun rviz rviz worked fast but had issues (fixed frame [map] does not exist. also not 
able to add robot model)
now rviz is open, has a robot model but does not show it, and says "NO PLANNING LIBRARY LOADED"

--------
7/9/2017
Error: package 'baxter_tools' not found
solution: source devel/setup.bash catkin_make try again

more thinking about final idea: Jordan suggests working off of previous studies, maybe even replicating
suggests deception, robots teaching humans (ie watching a video to learn how to make a burger vs learning
from robot how to make a burger at McDonalds) 

---------
7/12/2017
met with Holly, going to investigate various methods
of showing robot's intent before moving (compared
to not showing anything). 
papers to read: one by Andrea Thomaz has a graph
about reaction time that would be relevant

ideas about methods of communicating intent: 
- lights on arm that's going to move
- move head in direction (look)
- color (screen or led halo or sonar lights) -> can 
publish image to screen (can find/create solid color 
image) & change LED halo & choose to turn on specific
sonar lights
- pointing/little movement of arm that will move ->
have pointing and slight joint angle change in wrist
o sound/talking (maybe? creates expectation that 
robot is capable of verbal communication
o expression on face

when publishing pan_head commands, only need to 
publish once.

sonar leds: numbered corresponding to the sensor on its
right, except that the led corresponding to sensor 12 
is numbered as 0.

names for navigator lights: baxter website says 
left_itb_light_inner, etc but actually left_inner_light,
etc.

---------
7/14/2017
from Jordan about intent_demo code:
Although there are other problems in the code I believe
Generally try to avoid monolithic main statements\
better to group things into functions
also you create your subscribers
spinOnce, then initialize your start_pose hoping that the callback actually fired
Even if thats normally true, it's not a safe assumption
The best way to get the robots current position is using the Transform Frames (TF)
for example. tf.lookupFrame("/base", "left_hand") something like that,  lookup tf listeners
Also at the end you redpub.publish and greenpub.ppublish in the loop, but then you immedietly return from the program
if you don't spin() after publishing, it won't actually happen

tf tutorial: http://wiki.ros.org/tf/Tutorials/Writing%20a%20tf%20listener%20(C%2B%2B) 

Jordan continued: As an example, one function could be
geometry_msgs::PoseStamped InitializeMessage()
and you call it with start_pose_left = InitializeMessage()
something like that

---------
7/20/2017
all signalling functionality is done
looking into using Ein from Humans to Robots Lab at Brown for picking up objects
installing Ein.
http://h2r.github.io/ein/install/
actually, command line only :(

---------
7/24/2017
read DRC paper for lab meeting
finalized implementation of tf instead of subscriber for start_pose of each effector. 

for lab meeting: implemented intent signalling (except sound which would require external device), found Andrea Thomaz reaction time graph, found open source Baxter faces which we want to modify for gaze.

---------
7/25/2017
worked on poster for co-op presentation.

need to make a repo for everything from this summer before leaving (with a GOOD readme)
Code to upload:
- head turning based on sonar
- move arm service (and client?)
	- move arm (from above) service? Need to test first
- intent demo

--------
7/26/2017
git remote set-url origin https://Gal_Bejerano%40student.uml.edu@code.cs.uml.edu/scm/baxter/uml_baxter_tools.git
git push -u origin master
